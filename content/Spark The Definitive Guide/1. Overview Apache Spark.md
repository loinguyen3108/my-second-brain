---
aliases:
  - Overview Apache Spark
created: 2024-11-30
publish: 2024-11-30T22:07:00
progress: done
Author:
  - Loi Nguyen
Author Profile:
  - https://www.linkedin.com/in/loinguyen318/
blueprint:
  - "[[../../4. Blueprint/Apache Spark Knowledge|Apache Spark Knowledge]]"
  - "[[../../../4. Blueprint/Technical Knowledge|Technical Knowledge]]"
impact: 
tags:
  - data-engineer
  - spark
channel:
  - Blog
category:
  - technology
---
## Overview
To the spark perform work parallel, it split the data into chunks (partitions). A partition is collection of row or data, that sit on one physical machine in the cluster.

The core data structure is **immutable**, the spark do not carry out any transformation until we call an action. When an action have finished, It will return a new object and don't modify previous object

There are 2 types transformation:
- **narrow dependencies:** each input partition will contribute to only one output partition
![](../../../6.%20Vault/attachments/Pasted%20image%2020241102144435.png)
- **wide dependencies:** the input partition will contribute to many output partitions (**shuffle**)
![](../../../6.%20Vault/attachments/Pasted%20image%2020241102144755.png)
> [!Important] Shuffle
> Shuffle whereby Spark will exchange partitions across the cluster

> [!NOTE] How do the narrow and wide transformation work?
> - For the narrow transformation, the spark will automatically perform an operation call pipelining, that will performed in-memory.
> - For the wide transformation, the spark need to write the result to disk

In spark, **reading data is also transformation**, and is therefore a **lazy operation**
> [!NOTE] Sort
> Sort does not modify the DataFrame. We use sort as a transformation that returns a new DataFrame by transforming the previous DataFrame. Let’s illustrate what’s happening when we call take on that resulting DataFrame

![](../../../6.%20Vault/attachments/Pasted%20image%2020241102150301.png)
Spark uses an engine called Catalyst that maintains its own type information through the planning and processing of work. Spark types map directly to the different language APIs that Spark maintains and there exists a lookup table for each of these in Scala, Java, Python, SQL, and R.
## References
- [Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/) by Bill Chambers and Matei Zaharia.